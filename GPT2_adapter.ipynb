{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AdamW, GPT2Tokenizer, GPT2Model\n",
    "from utils.helper import load_model_recursive\n",
    "from ppcm_models.pytorch_pretrained_bert.modeling_adapter import GPT2LMHeadModel, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.model_size = 'medium'\n",
    "        self.lr = 2e-4 #, help=\"Learning rate\")\n",
    "        self.load_check_point_adapter = \"\"\n",
    "#         self.dataset_path = \"\" #\"Path or url of the dataset. If empty download from S3.\"\n",
    "#         self.dataset_cache = './dataset_cache' #, help=\"Path or url of the dataset cache\")\n",
    "#         self.model_checkpoint = \"gpt2\" #, help=\"Path, url or short name of the model\")\n",
    "#         self.num_candidates = 2 #, help=\"Number of candidates for training\")\n",
    "#         self.max_history = 15 #, help=\"Number of previous exchanges to keep in history\")\n",
    "#         self.max_seq_len = 200 #, help=\"Max number of tokens\")\n",
    "#         self.train_batch_size = 4 #, help=\"Batch size for training\")\n",
    "#         self.valid_batch_size = 4 #, help=\"Batch size for validation\")\n",
    "#         self.gradient_accumulation_steps = 8 #, help=\"Accumulate gradients on several steps\")\n",
    "#         self.max_norm = 1.0 #, help=\"Clipping gradient norm\")\n",
    "#         self.n_epochs = 5 #, help=\"Number of training epochs\")\n",
    "#         self.eval_before_start = 'store_true' #, help=\"If true start with a first evaluation before training\")\n",
    "#         self.device = 'cuda' if torch.cuda.is_available() else \"cpu\" #, help=\"Device (cuda or cpu)\")\n",
    "#         self.fp16 = \"\" #, help=\"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
    "#         self.local_rank = -1 #, help=\"Local rank for distributed training (-1: not distributed)\")\n",
    "#         self.debug = 'store_true' #, help=\"debugging mode\")\n",
    "#         self.dataset = 'SENT' #, help=\"Choose between SENT|TOXI|EMO|QUEST|TOPI \")\n",
    "#         self.label = 'very_negative' #, help=\"Choose between very_positive|very_negative|toxic|question\")\n",
    "#         self.kl_weight = 0 #, help=\"kl constraint for language model\")\n",
    "#         self.iter = 75 #, help=\"Load data from a certain iteration\")\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model from ppcm_models/dialoGPT/medium/medium_ft.pkl\n",
      "GPT2 loaded instead DialoGPT\n",
      "GPT2 param frozen, Adapter is trainable and initialized with AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/miniconda3/envs/env_composer/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args.model_path = f'ppcm_models/dialoGPT/{args.model_size}/'\n",
    "\n",
    "config = GPT2Config.from_json_file(os.path.join(args.model_path, 'config.json'))\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(args.model_path)\n",
    "\n",
    "## Load either Adapters' checkpoint, or just finetuned DialoGPT\n",
    "if(args.load_check_point_adapter != \"\"):\n",
    "    print(\"Loading ADAPTERS\")\n",
    "    model = load_model_recursive(GPT2LMHeadModel(config), args.load_check_point_adapter, args, verbose=True)\n",
    "else:\n",
    "    model = load_model_recursive(GPT2LMHeadModel(config), args.model_path+f\"{args.model_size}_ft.pkl\", args, verbose=True)\n",
    "\n",
    "## Load GPT2 instead of DialoGPT\n",
    "\n",
    "pt_gpt2_model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "\n",
    "model.transformer.wte.weight = pt_gpt2_model.wte.weight\n",
    "model.transformer.wpe.weight = pt_gpt2_model.wpe.weight\n",
    "\n",
    "layers = np.arange(0,len(pt_gpt2_model.h),1)\n",
    "for layer in layers:\n",
    "    model.transformer.h[layer].ln_1.weight = pt_gpt2_model.h[layer].ln_1.weight\n",
    "    model.transformer.h[layer].attn.c_attn.weight = pt_gpt2_model.h[layer].attn.c_attn.weight\n",
    "    model.transformer.h[layer].attn.c_proj.weight = pt_gpt2_model.h[layer].attn.c_proj.weight\n",
    "    model.transformer.h[layer].ln_2.weight = pt_gpt2_model.h[layer].ln_2.weight\n",
    "    model.transformer.h[layer].mlp.c_fc.weight = pt_gpt2_model.h[layer].mlp.c_fc.weight\n",
    "    model.transformer.h[layer].mlp.c_proj.weight = pt_gpt2_model.h[layer].mlp.c_proj.weight\n",
    "# model.to(args.device)\n",
    "print('GPT2 loaded instead DialoGPT')\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if \"adapter\" not in str(n):\n",
    "        p.requires_grad = False\n",
    "parameters_to_update = [p for n, p in model.named_parameters() if \"adapter\" in str(n)]\n",
    "optimizer = AdamW(parameters_to_update, lr=args.lr, correct_bias=True)\n",
    "print('GPT2 param frozen, Adapter is trainable and initialized with AdamW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check run\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "encoded_input = {'input_ids': encoded_input['input_ids']}\n",
    "output = model(**encoded_input, task_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# import jsonlines\n",
    "# from nltk import tokenize\n",
    "# from metric.lm_score import get_ppl\n",
    "\n",
    "# def make_data_loader(args,tokenizer):\n",
    "#     mapper = {\"very_negative\":\"results/sentiment_class_very_negative/sentiment_class_very_negative_iter_75_step_0.02_sample_10_wd_False_bce_False.jsonl\",\n",
    "#     \"very_positive\":\"results/sentiment_class_very_positive/sentiment_class_very_positive_iter_25_step_0.02_sample_10_wd_False_bce_False.jsonl\",\n",
    "#     \"toxic\":\"results/toxicity_class_toxic/toxicity_class_toxic_iter_75_step_0.02_sample_10_wd_False_bce_False.jsonl\",\n",
    "#     \"question\":\"results/daily_dialogue_act_class_question/daily_dialogue_act_class_question_iter_75_step_0.02_sample_10_wd_False_bce_False_1.jsonl\",\n",
    "#     \"Business\": \"results/AG_NEWS_class_Business/AG_NEWS_class_Business_iter_75_step_0.02_sample_10_wd_False_bce_False_1.jsonl\",\n",
    "#     \"SciTech\": \"results/AG_NEWS_class_SciTech/AG_NEWS_class_SciTech_iter_75_step_0.02_sample_10_wd_False_bce_False_1.jsonl\",\n",
    "#     \"Sports\": \"results/AG_NEWS_class_Sports/AG_NEWS_class_Sports_iter_75_step_0.02_sample_10_wd_False_bce_False_1.jsonl\",\n",
    "#     \"World\": \"results/AG_NEWS_class_World/AG_NEWS_class_World_iter_75_step_0.02_sample_10_wd_False_bce_False_1.jsonl\"\n",
    "#     }\n",
    "    \n",
    "#     f = mapper[args.label]\n",
    "#     response = []\n",
    "#     with jsonlines.open(f) as reader: \n",
    "#         for i, obj in enumerate(reader):\n",
    "#             text = \" \".join(tokenize.sent_tokenize(obj[\"hyp\"][\"PPLM\"][0][-1])[:2])\n",
    "#             score = get_ppl(text)\n",
    "#             if score>700:\n",
    "#                 continue\n",
    "#             response.append(obj['conversation']['conversation']+[text])\n",
    "            \n",
    "#     dataset = []\n",
    "#     for r in response:\n",
    "#         seq = build_input_from_segments(args, r[:-1], r[-1], tokenizer)\n",
    "#         dataset.append(seq)\n",
    "#     train_dataset = DatasetTrain(dataset)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "#     return train_loader\n",
    "\n",
    "# train_loader = make_data_loader(args, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_composer",
   "language": "python",
   "name": "env_composer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
