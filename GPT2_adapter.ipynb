{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import AdamW, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "from ppcm_models.pytorch_pretrained_bert.modeling_adapter import GPT2LMHeadModel, GPT2Config\n",
    "from utils.helper import load_model_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataArguments():\n",
    "    def __init__(self):\n",
    "        self.dataset_path = '/home/bryan/datasets/bookcorpusopen/bookcorpusopen_chunked.arrow'\n",
    "        self.bookcorpusopen_story_column_name = 'chunk'\n",
    "        self.preprocessing_num_workers = 1\n",
    "        \n",
    "class ModelArguments():\n",
    "    def __init__(self):\n",
    "        self.model_size = 'medium'\n",
    "        self.lr = 2e-4 #, help=\"Learning rate\")\n",
    "        self.load_check_point_adapter = \"\"\n",
    "#         self.dataset_path = \"\" #\"Path or url of the dataset. If empty download from S3.\"\n",
    "#         self.dataset_cache = './dataset_cache' #, help=\"Path or url of the dataset cache\")\n",
    "#         self.model_checkpoint = \"gpt2\" #, help=\"Path, url or short name of the model\")\n",
    "#         self.num_candidates = 2 #, help=\"Number of candidates for training\")\n",
    "#         self.max_history = 15 #, help=\"Number of previous exchanges to keep in history\")\n",
    "#         self.max_seq_len = 200 #, help=\"Max number of tokens\")\n",
    "#         self.train_batch_size = 4 #, help=\"Batch size for training\")\n",
    "#         self.valid_batch_size = 4 #, help=\"Batch size for validation\")\n",
    "#         self.gradient_accumulation_steps = 8 #, help=\"Accumulate gradients on several steps\")\n",
    "#         self.max_norm = 1.0 #, help=\"Clipping gradient norm\")\n",
    "#         self.n_epochs = 5 #, help=\"Number of training epochs\")\n",
    "#         self.eval_before_start = 'store_true' #, help=\"If true start with a first evaluation before training\")\n",
    "#         self.device = 'cuda' if torch.cuda.is_available() else \"cpu\" #, help=\"Device (cuda or cpu)\")\n",
    "#         self.fp16 = \"\" #, help=\"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
    "#         self.local_rank = -1 #, help=\"Local rank for distributed training (-1: not distributed)\")\n",
    "#         self.debug = 'store_true' #, help=\"debugging mode\")\n",
    "#         self.dataset = 'SENT' #, help=\"Choose between SENT|TOXI|EMO|QUEST|TOPI \")\n",
    "#         self.label = 'very_negative' #, help=\"Choose between very_positive|very_negative|toxic|question\")\n",
    "#         self.kl_weight = 0 #, help=\"kl constraint for language model\")\n",
    "#         self.iter = 75 #, help=\"Load data from a certain iteration\")\n",
    "        \n",
    "model_args = ModelArguments()\n",
    "data_args = DataArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model from ppcm_models/dialoGPT/medium/medium_ft.pkl\n",
      "GPT2 loaded instead DialoGPT\n",
      "GPT2 param frozen, Adapter is trainable and initialized with AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/miniconda3/envs/pix2story/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_args.model_path = f'ppcm_models/dialoGPT/{model_args.model_size}/'\n",
    "\n",
    "config = GPT2Config.from_json_file(os.path.join(model_args.model_path, 'config.json'))\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_args.model_path)\n",
    "\n",
    "## Load either Adapters' checkpoint, or just finetuned DialoGPT\n",
    "if(model_args.load_check_point_adapter != \"\"):\n",
    "    print(\"Loading ADAPTERS\")\n",
    "    model = load_model_recursive(GPT2LMHeadModel(config), model_args.load_check_point_adapter, model_args, verbose=True)\n",
    "else:\n",
    "    model = load_model_recursive(GPT2LMHeadModel(config), model_args.model_path+f\"{model_args.model_size}_ft.pkl\", model_args, verbose=True)\n",
    "\n",
    "## Load GPT2 instead of DialoGPT\n",
    "\n",
    "pt_gpt2_model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "\n",
    "model.transformer.wte.weight = pt_gpt2_model.wte.weight\n",
    "model.transformer.wpe.weight = pt_gpt2_model.wpe.weight\n",
    "\n",
    "layers = np.arange(0,len(pt_gpt2_model.h),1)\n",
    "for layer in layers:\n",
    "    model.transformer.h[layer].ln_1.weight = pt_gpt2_model.h[layer].ln_1.weight\n",
    "    model.transformer.h[layer].attn.c_attn.weight = pt_gpt2_model.h[layer].attn.c_attn.weight\n",
    "    model.transformer.h[layer].attn.c_proj.weight = pt_gpt2_model.h[layer].attn.c_proj.weight\n",
    "    model.transformer.h[layer].ln_2.weight = pt_gpt2_model.h[layer].ln_2.weight\n",
    "    model.transformer.h[layer].mlp.c_fc.weight = pt_gpt2_model.h[layer].mlp.c_fc.weight\n",
    "    model.transformer.h[layer].mlp.c_proj.weight = pt_gpt2_model.h[layer].mlp.c_proj.weight\n",
    "# model.to(model_args.device)\n",
    "print('GPT2 loaded instead DialoGPT')\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if \"adapter\" not in str(n):\n",
    "        p.requires_grad = False\n",
    "parameters_to_update = [p for n, p in model.named_parameters() if \"adapter\" in str(n)]\n",
    "optimizer = AdamW(parameters_to_update, lr=model_args.lr, correct_bias=True)\n",
    "print('GPT2 param frozen, Adapter is trainable and initialized with AdamW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BookcorpusGenreAdapterDataset(Dataset):\n",
    "    def __init__(self, data_args, split, tokenizer, genres=None, sample_row=100,\n",
    "                        top_n_genres=6, genre_sample_range=100, max_seq_len=512,\n",
    "                         exclude_non_adapter=True, truncate=True, \n",
    "                         add_special_tokens = True,\n",
    "                         *args, **kwargs):\n",
    "        super(BookcorpusGenreAdapterDataset, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.data_args = data_args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "        self.truncate = truncate\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.preprocessing_num_workers = data_args.preprocessing_num_workers\n",
    "        self.tokenized_dataset, self.genres = self.load_bookcorpusopen(split, genres, sample_row, \n",
    "                                                                top_n_genres, genre_sample_range,\n",
    "                                                                exclude_non_adapter)\n",
    "\n",
    "    def load_bookcorpusopen(self, split, genres=None, sample_row=None, top_n_genres=None, \n",
    "                            genre_sample_range=None, exclude_non_adapter=True):\n",
    "        \"\"\"\n",
    "        Load bookcorpusopen from pyarrow file.\n",
    "            \n",
    "        Args:\n",
    "            split: string, {train, valid, test}\n",
    "            genres: list of string, genres that we want the dataset to be labelled with, \n",
    "                    according to the index, e.g. ['Fiction', 'General', ...]\n",
    "            sample_row: int, set the int number to sample the dataset, None means using \n",
    "                        all the datasets samples available\n",
    "            top_n_genres: int, if genres==None, we will extract the list of genres ourselves,\n",
    "                          and top_n_genres dictates how many genres we want to take sorted on\n",
    "                          the frequency, descending\n",
    "            genre_sample_range: int, if genres==None, we will extract the list of genres ourselves,\n",
    "                                and genre_sample_range dictates how many samples will be used in to\n",
    "                                derive list of genres\n",
    "            exclude_non_adapter: bool, set to False if we want to use the non styled dataset\n",
    "            \n",
    "        Returns:\n",
    "            dataset: tokenized huggingface dataset format from one of the bookcorpusopen split, \n",
    "                        with the adapter_id attached, and without any adapter_id = -1\n",
    "        \"\"\"\n",
    "        \n",
    "        def get_adapter_id(story_genre_list_string, adapter_genre_list):\n",
    "            \"\"\"\n",
    "            assume that the genre of story is the foremost genre listed in story_genre_list_string\n",
    "            \"\"\"\n",
    "            spotted_genre = {}\n",
    "            selected_adapter_id = -1\n",
    "            genre_index = 9999999999999999\n",
    "            story_genre_list = [genre[1:-1] for genre in story_genre_list_string[1:-1].split(', ')]\n",
    "\n",
    "            for adapter_id, adapter_genre in enumerate(adapter_genre_list):\n",
    "                if adapter_genre.lower() in story_genre_list_string.lower():\n",
    "                    for i, story_genre in enumerate(story_genre_list):\n",
    "                        if adapter_genre in story_genre_list and i < genre_index:\n",
    "                            genre_index = i\n",
    "                    spotted_genre[i] = adapter_id\n",
    "            selected_adapter_id = spotted_genre[min(spotted_genre.keys())] \\\n",
    "                                    if len(spotted_genre)>0 else selected_adapter_id\n",
    "\n",
    "            return selected_adapter_id\n",
    "        \n",
    "        def map_tokenization(batch):\n",
    "            tokenized = tokenizer(batch[data_args.bookcorpusopen_story_column_name], \n",
    "                                  truncation=self.truncate,\n",
    "                                  max_length=self.max_seq_len,\n",
    "                                  add_special_tokens=self.add_special_tokens,\n",
    "                                  return_tensors='pt')\n",
    "            return tokenized\n",
    "        \n",
    "        # load bookcorpusopen from arrow file\n",
    "        datasets = DatasetDict()\n",
    "        print('Loading train, validation, test dataset...')\n",
    "        datasets = load_from_disk(data_args.dataset_path)\n",
    "        print('Loaded')\n",
    "        sample_row = len(datasets[split]) if sample_row == None else sample_row\n",
    "        \n",
    "        print('Getting adapter_ids and use only the selected split')\n",
    "        # if frequent_genres not defined yet, derive frequent genres\n",
    "        if genres == None:\n",
    "            print('Generating new list of frequent genres from', split, 'split')\n",
    "            genres=[]\n",
    "            for i in range(sample_range):\n",
    "                genres.extend([genre[1:-1] for genre in datasets[split]['genre'][i][1:-1].split(', ')])\n",
    "\n",
    "            df_genres = pd.DataFrame({'genres':genres})\n",
    "            frequent_genres = df_genres.genres.value_counts()[:top_n].index.tolist()\n",
    "            frequent_genres.remove('') if '' in top_genres else top_genres\n",
    "            genres = frequent_genres\n",
    "\n",
    "        dataset = datasets[split].select(np.arange(0,sample_row,1))\\\n",
    "                                    .map(lambda x: {'adapter_id': get_adapter_id(x['genre'], genres)}\\\n",
    "                                         , num_proc=self.preprocessing_num_workers)\n",
    "        dataset = dataset.filter(lambda x: x['adapter_id']!=-1) if exclude_non_adapter else dataset\n",
    "        print('Derived adapter_ids and used only the', split, 'split')\n",
    "        \n",
    "        # Tokenize with huggingface datasets mapping function\n",
    "        tokenized_dataset = dataset.map(\n",
    "            map_tokenization,\n",
    "            remove_columns=data_args.bookcorpusopen_story_column_name,\n",
    "            num_proc=self.preprocessing_num_workers,\n",
    "            load_from_cache_file=True\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset, genres\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            \n",
    "        forward_inputs = {}\n",
    "        forward_inputs['input_ids'] = self.tokenized_dataset[index]['input_ids']\n",
    "        forward_inputs['attention_mask'] = self.tokenized_dataset[index]['attention_mask']\n",
    "        forward_inputs['adapter_id'] = self.tokenized_dataset[index]['adapter_id']\n",
    "        return forward_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokenized_dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train, validation, test dataset...\n",
      "Loaded\n",
      "Getting adapter_ids and use only the selected split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39b9fd020ab479da5923f83487b46df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37f16f665a94d80bbc897c8a6a9e8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived adapter_ids and used only the train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486e47744c624ba883ca73aeafcdb637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequent_genres = ['Fiction', 'General', 'Fantasy', 'Romance', 'Adventure']\n",
    "train_dataset = BookcorpusGenreAdapterDataset(data_args, 'train', tokenizer, genres=frequent_genres,\n",
    "                                                sample_row=200, top_n_genres=6, genre_sample_range=100, \n",
    "                                                  max_seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check run\n",
    "\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# encoded_input = {'input_ids': encoded_input['input_ids']}\n",
    "# output = model(**encoded_input, task_id=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "\n",
    "- Set up the training code to train the adapters\n",
    "- Trial run the adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pix2story",
   "language": "python",
   "name": "pix2story"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
