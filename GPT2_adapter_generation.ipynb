{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6b6463-e3e4-4def-9414-431f366b0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, TrainingArguments\n",
    "from utils.helper import load_model_recursive\n",
    "from ppcm_models.pytorch_pretrained_bert.modeling_adapter import GPT2LMHeadModel, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dbf6887-a0c8-4ff8-b3e0-d0ff9b659bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataArguments():\n",
    "    def __init__(self):\n",
    "        self.dataset_path = '/home/bryan/datasets/bookcorpusopen/bookcorpusopen_chunked.arrow'\n",
    "        self.bookcorpusopen_story_column_name = 'chunk'\n",
    "        self.preprocessing_num_workers = 8\n",
    "        self.genre='Romance'\n",
    "        self.adapter_id=1\n",
    "        self.match_up_to_n_genres=3\n",
    "        self.sample_row=None\n",
    "        \n",
    "class ModelArguments():\n",
    "    def __init__(self):\n",
    "        self.model_size = 'small'\n",
    "        self.load_checkpoint_adapter = \"\"\n",
    "        self.max_seq_len=512\n",
    "        # self.lr = 2e-4 #, help=\"Learning rate\")\n",
    "\n",
    "class TrainingArguments(TrainingArguments):\n",
    "    def __init__(self):\n",
    "        self.output_dir = \"./save\"\n",
    "        self.eval_accumulation_steps = None\n",
    "        \n",
    "model_args = ModelArguments()\n",
    "data_args = DataArguments()\n",
    "training_args = TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd69338-00ae-4e00-b2ac-1d19e981b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.model_path = f'ppcm_models/dialoGPT/small/'\n",
    "config = GPT2Config.from_json_file(os.path.join(model_args.model_path, 'config.json'))\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608d510c-136b-483a-99e7-511548100827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model from ./save/GPT2small_adapterid0_genreAction_matched3_sampleNone_maxseqlen512_bs8_lr5e-05_10.0epoch_wd0.0_ws0/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/miniconda3/envs/pix2story/lib/python3.10/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/home/bryan/miniconda3/envs/pix2story/lib/python3.10/site-packages/transformers/generation_beam_search.py:196: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my dog is cute. I'm going to take her to the vet. She's a little bit scared. But I'll be okay.\"\"I'm sorry, but I don't know what to do. It's not like I want to be in a relationship with you. You're my best friend, and I love you.\"The man was a man of few words. He was tall, thin, with a thick, dark complexion. His hair was long and flowing. The<|endoftext|> \n",
      "\n",
      "Loading finetuned model from ./save/GPT2small_adapterid0_genreAction_matched3_sampleNone_maxseqlen512_bs8_lr5e-05_2.0epoch_wd0.0_ws0/pytorch_model.bin\n",
      "Hello, my dog is cute. I'm not sure if she's cute or not. But I think she is. She's a little bit cute, but I don't think I can tell you what she looks like.\"I'm sorry, I was just trying to get you to stop talking about me. It's not like I've ever talked to you before. You're a good guy. And I know you're not.\"\"You're right. We're in the middle of<|endoftext|> \n",
      "\n",
      "Loading finetuned model from ./save/GPT2small_adapterid0_genreMystery_matched3_sampleNone_maxseqlen512_bs8_lr0.0005_5.0epoch_wd0.0_ws0/pytorch_model.bin\n",
      "Hello, my dog is cute. I'm not sure if she's a dog or a cat. She's just a little girl. But she loves me. And I love her so much.\"\"I'm sorry, but I don't know what to do. It's not like I want to be here. You know, I've been here a few times. The last time I was here was in the middle of the night. So I can't be there.\"The next morning,<|endoftext|> \n",
      "\n",
      "Loading finetuned model from ./save/GPT2small_adapterid0_genreRomance_matched3_sampleNone_maxseqlen512_bs8_lr5e-05_2.0epoch_wd0.0_ws0/pytorch_model.bin\n",
      "Hello, my dog is cute. I'm not sure if she's a good dog, but she is a very good one. She's very obedient and she loves to be around me.\"\"I'm sorry, I didn't mean to. It's just that I don't want to see you again. You're a little too young to know what I mean.\" He looked at her and then back at me. \"I don' think I can handle it.\"He was a man of<|endoftext|> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_run_names = ['GPT2small_adapterid0_genreAction_matched3_sampleNone_maxseqlen512_bs8_lr5e-05_10.0epoch_wd0.0_ws0',\n",
    "                   'GPT2small_adapterid0_genreAction_matched3_sampleNone_maxseqlen512_bs8_lr5e-05_2.0epoch_wd0.0_ws0',\n",
    "                   'GPT2small_adapterid0_genreMystery_matched3_sampleNone_maxseqlen512_bs8_lr0.0005_5.0epoch_wd0.0_ws0',\n",
    "                   'GPT2small_adapterid0_genreRomance_matched3_sampleNone_maxseqlen512_bs8_lr5e-05_2.0epoch_wd0.0_ws0']\n",
    "\n",
    "for i, model_run_name in enumerate(model_run_names):\n",
    "\n",
    "    path = './save/'+model_run_name+'/pytorch_model.bin'\n",
    "    model = load_model_recursive(GPT2LMHeadModel(config), path, model_args, verbose=True)\n",
    "\n",
    "    # # use this to generate outputs\n",
    "    # inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "    # outputs = model(inputs['input_ids'], task_id=[0])\n",
    "    # # or\n",
    "    # outputs = model.transformer(inputs['input_ids'], task_id=0)\n",
    "\n",
    "    length = 100\n",
    "    text = \"Hello, my dog is cute\"\n",
    "    generated = tokenizer.encode(text)\n",
    "    context = torch.tensor([generated])\n",
    "\n",
    "    generation = model.generate(inputs=context,\n",
    "                               num_beams=3, \n",
    "                               length_penalty=3, \n",
    "                               early_stopping=1, \n",
    "                               num_beam_groups=3, \n",
    "                               do_sample=False, \n",
    "                               num_return_sequences=2, \n",
    "                               bos_token_id=50256,\n",
    "                               eos_token_id=50256,\n",
    "                               pad_token_id=50256,\n",
    "                               output_scores=True,\n",
    "                               output_attentions=True,\n",
    "                               output_hidden_states=True,\n",
    "                               return_dict_in_generate=True,\n",
    "                               repetition_penalty=1.1,\n",
    "                               min_length = 0,\n",
    "                               max_length = length,\n",
    "                               no_repeat_ngram_size=2,\n",
    "                               encoder_no_repeat_ngram_size=False,\n",
    "                               bad_words_ids=[[100]], # tokenizer.decode(100)\n",
    "                               diversity_penalty=0.2,\n",
    "                               forced_bos_token_id=50256,\n",
    "                               forced_eos_token_id=50256,\n",
    "                               remove_invalid_values=True,\n",
    "                               exponential_decay_length_penalty=[1.0, 1.2])\n",
    "\n",
    "    print(tokenizer.decode(generation[0][0]), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pix2story",
   "language": "python",
   "name": "pix2story"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
